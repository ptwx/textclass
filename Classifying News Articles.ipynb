{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying News Articles\n",
    "\n",
    "Created by Wei Xin Tan\n",
    "\n",
    "### Method: TF-IDF + Stacked LDA (Latent Dirichlet Allocation) + SWM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: NLP\n",
      "stm v1.3.3 (2018-1-26) successfully loaded. See ?stm for help. \n",
      " Papers, resources, and other materials at structuraltopicmodel.com\n",
      "\n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "library(e1071)\n",
    "library(tm)\n",
    "library(lda)\n",
    "library(stm)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Macro function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro.f1 <- function(predicted,true.label){\n",
    "    \n",
    "    predicted <- as.integer(predicted);true.label <- as.integer(true.label)   \n",
    "    classes <- unique(true.label)\n",
    "    no.classes  <- length(classes)\n",
    "    f1 <- 0\n",
    "\n",
    "    for (class in classes){ \n",
    "        actual.positive <- sum(true.label == class)\n",
    "        true.positive <- sum((predicted == true.label)&(predicted == class))\n",
    "        total.predicted  <- sum(predicted == class)\n",
    "        recall <- true.positive/actual.positive\n",
    "        precision <- true.positive/total.predicted\n",
    "        temp.f1 <- (2*((recall*precision)/(recall+precision)))\n",
    "        #temp.f1 <- ifelse(is.na(temp.f1), 0, temp.f1)\n",
    "        f1 <- f1 + temp.f1    \n",
    "\n",
    "    }\n",
    "    macro.f1 <- f1/no.classes\n",
    "    return(macro.f1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading text and extract each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data\n",
    "train.text <- readLines('./training_docs.txt')\n",
    "train.text <- train.text[train.text > 0 & train.text != 'EOD']\n",
    "train.docs <- data.frame('doc_id'=train.text[seq(1,length(train.text),2)],'text'=train.text[seq(2,length(train.text),2)])\n",
    "train.label <- read.table('./training_labels_final.txt',stringsAsFactors=FALSE)\n",
    "names(train.label) <- c('document','class')\n",
    "\n",
    "# read test data\n",
    "test.text <- readLines('./testing_docs.txt')\n",
    "test.text <- test.text[test.text > 0 & test.text != 'EOD']\n",
    "test.docs <- data.frame('doc_id'=test.text[seq(1,length(test.text),2)],'text'=test.text[seq(2,length(test.text),2)])\n",
    "cut.point <- nrow(test.docs)\n",
    "\n",
    "docs <- rbind(test.docs,train.docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a corpus\n",
    "docs <- DataframeSource(docs)\n",
    "docs <- Corpus(docs)\n",
    "\n",
    "# Preprocessing:\n",
    "docs <- tm_map(docs, removeNumbers) # remove all numbers\n",
    "docs <- tm_map(docs, stripWhitespace) # remove redundant spaces \n",
    "docs <- tm_map(docs, content_transformer(tolower))\n",
    "docs <- tm_map(docs, removeWords, c('text','will',\"say\", \"said\", 'can','saying','tell', stopwords(\"english\"))) # remove stop words (the most common word in a language that can be find in any document)\n",
    "docs <- tm_map(docs, removePunctuation) # remove punctuation\n",
    "docs <- tm_map(docs, stemDocument) # perform stemming (reducing inflected and derived words to their root form)\n",
    "docs <- tm_map(docs, removeWords, c('text','will',\"say\", \"said\", 'can','saying','tell', stopwords(\"english\"))) \n",
    "\n",
    "# dtm for lda\n",
    "lda.dtm <- DocumentTermMatrix(docs)\n",
    "\n",
    "# dtm for svm\n",
    "svm.dtm <- DocumentTermMatrix(docs, control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))\n",
    "svm.dtm <- removeSparseTerms(svm.dtm, 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda.dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm.dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Topic modelling with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further processing to LDA required data\n",
    "minusone <- function(item){\n",
    "    return(matrix(as.integer(c(item[1,] - 1,item[2,])),2,,byrow = TRUE))\n",
    "    }\n",
    "\n",
    "slam.dtm <- readCorpus(lda.dtm, type = \"slam\")\n",
    "rowTotals <- lapply(slam.dtm$documents, sum) #Find the sum of words in each Document\n",
    "slam.dtm$documents  <- slam.dtm$documents[rowTotals > 0]\n",
    "slam.dtm$documents <- lapply(slam.dtm$documents,minusone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train LDA\n",
    "num.topics <- 23\n",
    "result <- lda.collapsed.gibbs.sampler(slam.dtm$documents,\n",
    "                                      num.topics,  ## Num clusters\n",
    "                                      slam.dtm$vocab,\n",
    "                                      num.iterations = 100,  ## Num iterations\n",
    "                                      alpha = 50/num.topics,\n",
    "                                      eta = 0.1,\n",
    "                                      compute.log.likelihood=FALSE,\n",
    "                                      trace=1L)\n",
    "# Getting the Gammas/Posteriors\n",
    "gamma <- t(result$document_sums) / colSums(result$document_sums)\n",
    "gamma.df <- data.frame(names(slam.dtm$documents),gamma) # check this order\n",
    "colnames(gamma.df) <- c('document',1:num.topics)\n",
    "gamma.df$document <- gsub('ID ','', gamma.df$document)\n",
    "grab.indx <- grep('te+',gamma.df$document)\n",
    "lda.test.data <- gamma.df[grab.indx,]\n",
    "lda.train.data <- gamma.df[-grab.indx,]\n",
    "\n",
    "for (k in c(55,110,160,210,260)){\n",
    "    message('Training',k)\n",
    "    # Training LDA\n",
    "    num.topics <- k\n",
    "    result <- lda.collapsed.gibbs.sampler(slam.dtm$documents,\n",
    "                                          num.topics,  ## Num clusters\n",
    "                                          slam.dtm$vocab,\n",
    "                                          num.iterations = 100,  ## Num iterations\n",
    "                                          alpha = 50/num.topics,\n",
    "                                          eta = 0.1,\n",
    "                                          compute.log.likelihood=FALSE,\n",
    "                                          trace=1L)\n",
    "    # Getting the Gammas/Posteriors\n",
    "    gamma <- t(result$document_sums) / colSums(result$document_sums)\n",
    "    gamma.df <- data.frame(names(slam.dtm$documents),gamma) # check this order\n",
    "    colnames(gamma.df) <- c('document',1:num.topics)\n",
    "    gamma.df$document <- gsub('ID ','', gamma.df$document)\n",
    "    grab.indx <- grep('te+',gamma.df$document)\n",
    "    lda.test.data <- cbind(lda.test.data,gamma.df[grab.indx,-1])\n",
    "    lda.train.data <- cbind(lda.train.data,gamma.df[-grab.indx,-1])\n",
    "    }\n",
    "\n",
    "# rename the columns\n",
    "colnames(lda.train.data) <- c('document',1:818)\n",
    "colnames(lda.test.data) <- c('document',1:818)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Merging the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract tfidf data for training svm\n",
    "m <- as.matrix(svm.dtm)\n",
    "df.m <- data.frame(document=rownames(m),m,row.names = NULL,stringsAsFactors=FALSE)\n",
    "df.m$document <- gsub('ID ','', df.m$document)\n",
    "svm.test.data <- df.m[1:cut.point,]\n",
    "svm.all.train.data <- df.m[-(1:cut.point),]\n",
    "\n",
    "# Merging the posterior from lda and tf-dif\n",
    "new.data <- merge(lda.train.data , svm.all.train.data , by=\"document\",all=TRUE)\n",
    "new.data [is.na(new.data )] <- 0\n",
    "new.data.w.label <- merge(new.data , train.label , by=\"document\")\n",
    "new.label <- new.data.w.label[,ncol(new.data.w.label)]\n",
    "new.features <- new.data.w.label[,2:(ncol(new.data.w.label)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Sampling and spliting data into train and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample portion of the data to train our classifier\n",
    "sam.idx <- sample(1:nrow(train.docs),60000)\n",
    "new.features <- new.features[sam.idx,]\n",
    "new.label <- new.label[sam.idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the index according to defined percentage.\n",
    "smp_size <- floor(0.75 * nrow(new.features))\n",
    "train_ind <- sample(seq_len(nrow(new.features)), size = smp_size)\n",
    "\n",
    "# Features set\n",
    "to.train.data <- new.features[train_ind, ]\n",
    "val.data <- new.features[-train_ind, ]\n",
    "\n",
    "# Label set\n",
    "to.train.label <- as.factor(new.label[train_ind])\n",
    "val.label <- as.factor(new.label[-train_ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training SVM multi-class classifier (One vs One) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "svm_model <- svm(x=to.train.data,y=to.train.label,method=\"C-classification\", kernel=\"sigmoid\",cachesize = 400)\n",
    "pred <-predict(svm_model,val.data)\n",
    "macro.f1(pred,val.label)\n",
    "#system('say -r 180 hey jarvis, your model has finished training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test set \n",
    "new.test.data <- merge(lda.test.data , svm.test.data , by=\"document\",all=TRUE)\n",
    "new.test.data [is.na(new.test.data )] <- 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minor reordering\n",
    "test.pred <- predict(svm_model, new.test.data[-1])\n",
    "pred.df <- data.frame('doc'=new.test.data$document,'pred'=test.pred,'idx'=as.numeric(gsub('te_doc_','', new.test.data$document)))\n",
    "pred.df <- pred.df[order(pred.df$idx),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the prediction\n",
    "to.write <- pred.df[,c('doc','pred')]\n",
    "write.table(to.write, file = \"testing_labels_pred.txt\",quote = FALSE,row.names = FALSE, col.names=FALSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
